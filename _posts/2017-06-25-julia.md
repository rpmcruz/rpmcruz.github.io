---
layout: post
title: "Julia: a first look"
category: machine learning
---

I am a heavy Python user when it comes to machine learning (and several other things). I occasionally use R, but I prefer Python because its syntax are nicer for me, as someone who is used to mainstream languages like C++ and Java, and it has a very powerful ecosystem: especially [scikit-learn.](http://scikit-learn.org/) However, it is SLOW...

Before going forward, let me clarify. Most people use Python has a simple wrapper to packages which are compiled into machine code. Numerical packages like Numpy and Scipy are implemented in C and Fortran. Therefore, if you can vectorize your code, that is use linear algebra as much as possible, then your code will run almost as fast as native code.

The problem is that some code cannot be vectorized. Sometimes you need to use cycles and, in those cases, Python is indeed SLOW.

There are actually several implementations of Python:

- CPython, whose source code --(is compiled into)--> bytecode and then --(executed)--> in its own runtime.
- Jython, whose source code --(is compiled into)--> Java bytecode and then --(executed)--> in the Java runtime.
- Pypy, whose source code --(is compiled into)--> bytecode and then --(executed)--> in the its own runtime and code --(is compiled into)--> machine code at runtime (i.e. it features a JIT: Just In Time compiler like Java and MATLAB).

Notice I have used the generic word *runtime* instead of *interpreter* or *virtual machine* because I want to avoid definitional wars. I think modern languages have features that encompass all these things, and it's impossible to narrow down them their respective runtimes into one simple category.

When people talk about Python, they are talking about the official CPython implementation of the language. This implementation can be speed up through such means as:

- Cython: some portions of the code can be compiled into machine code.
- ctypes: loads a C shared object and uses its functions as native.
- [NUMBLA](http://numba.pydata.org/): decorators whose functions get JIT'd.

The latter technique seems powerful and incredibly easy, but I never used it.

## Here comes Julia

[Julia](https://julialang.org/) promises the speed of a machine compiled language and all the advantages of dynamic programming languages.

How is that possible? Simple, code gets compiled at runtime. People usually associate dynamic typing and other techniques to interpreted languages. This need not be so.

I was able to reduce the training time of a support vector machine I implemented in Python, from over **5 minutes** to less than **15 seconds** in Julia. In bigger datasets, hours of runtime got reduced to a few minutes.

Of course, benchmarking is unfair because it is impossible to implement an algorithm in exact the same way in every language. The syntax is different and the performance of the algorithm will be reflected by the programmer's experience with the language. But, in this case, it should have worked the other way around. Code that I was able to vectorize in Python, I was unable to do so in Julia. Still, Julia ran faster.

Now that I have convinced that performance-wise Julia is a good bet, let me sell you on its easeness-of-use. Judging from the github discussions, it is clear that Julia's developers are knwoledgeable of Python/Numpy and R. I never saw MATLAB mentioned, which is strange because the language felt a lot like MATLAB to me. This is a comparison between Julia and the other languages:

| Python/Numpy | R            | MATLAB       | Julia        |
| ------------ | ------------ | ------------ | ------------ |
| X.shape      | size(X),nrow(X),ncol(X) | size(X) | size(X)|
| X[0:5,:],X[0:5],X[:5] | X[1:4,] | X(1:4,:) | X[1:4,:]     |
| X[-5:]       | X[nrow(X)-4:nrow(X),] | X(end-4:end,:) | X[size(X,1)-4:size(X,1),:] |
| X.transpose(),X.T | t(X)    | X.'          | X.'          |
| X.dot(Y)     | X %\*% Y     | X\*Y         | X\*Y         |
| X\*Y         | X\*Y         | X.\*Y        | X.\*Y        |

Notes:

- element-wise arithmetic is default in Numpy and R, but in MATLAB and Julia you need to use dot-operator for element-wise operations.
- R, MATLAB and Julia start indexing on 1, and intervals are close in both sides [a,b].

A couple of other big notes, not inferred from the table:

- scope is defined in Python by the colons and the identation, in R by brackets, and in MATLAB and Julia is delimited by the keywords `end`.
- unlike MATLAB, there is a `return` keyword, and, like R, the last statement is automatically returned.
- in R, MATLAB and Julia when importing a package, the global namespace is polluted, while in Python you are advised to import only the names you want (I actually prefer the former approach, even if name collision becomes an hazard)
- Numpy supports arithmetic broadcasting. That is, when doing element-wise multiplication between two matrices of shape (10,3) and (10,1), it automatically repeats the last axis. In MATLAB you would need to use `repmat` to repeat the last dimension three times. In Julia, there is no implicit broadcast, but you can use the `broadcast` keyword to use it.
- Julia also comes by default with a bunch of functions like tic/toc.

## Multiprocessing in Julia

Julia by default uses threads, not processes, and provides a very powerful framework to work with them.

But I do not need a powerful framework, I just want to run a dataset or an algorithm in each worker, and get a number showing how well my algorithm worked. I don't need any communication or anything fancy. Just have each worker working on its own thing.

POSIX `fork()` semantics makes this very easy, since the entire data space is shallow-copied to each child, you can easily create a pool of workers that already know everything they need to work. There is no need to manually copy data to each worker. (And, since Linux uses intelligent copy-on-write, as long as you only need to read the datasets, no copy penalty is ever performed.)

This is very easy using the `multiprocessing` package in Python, which uses `fork()` behind the scenes:

```python
import numpy as np

# import dataset
dataset = np.loadtxt(...)

# I have several algorithms
algorithms = [
    Algorithm1(),
    Algorithm2(),
    Algorithm3(),
]

# parallelize the algorithms
def test(algorithm):
    a = algorithm.run(dataset)
    return evaluate(algorithm)

import multiprocessing
pool = multiprocessing.Pool(4)
print(pool.map(test, algorithms))
```

Unfortunately, Julia prefers to use threads rather than forks, because Windows is not POSIX-compliant and does not support `fork()`. Threads I think are also more flexible, which I personally don't care for.

This is how I managed to get this working on Julia. [Several approach are possible](https://stackoverflow.com/questions/44741667/julia-equivalent-of-python-multiprocessing-pool-map/44742371), but I have found that taking advantage of the fact that local variables are automatically copied over to the workers make more sense:

```julia
# we need to create several pthreads right away
addprocs(3)

function main() 
    # import dataset
    dataset = loadcsv(...)

    # I have several algorithms
    algorithms = [
        Algorithm1(),
        Algorithm2(),
        Algorithm3(),
    ]

    # parallelize the algorithms
    function test(algorithm)
        a = algorithm.run(dataset)
        evaluate(algorithm)
    end

    println(pmap(test, algorithms))
end

main()
```

Otherwise, you would need to use `@everywhere` everywhere to indicate that code is to be run by all processes. You'll still need this to define the structures you want, but you can use `@everywhere` for blocks, as in:

```julia
@everywhere begin
    type Model
        param
    end
end
```
